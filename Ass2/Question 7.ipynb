{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8577b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbd4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states= np.array(range(25))\n",
    "v_pi=np.zeros(25)\n",
    "gamma=0.9\n",
    "actions= ['left', 'up', 'right', 'down']\n",
    "\n",
    "#Initially in all states, every action is equi probable.\n",
    "pi_s= np.ones((25,4))*0.25\n",
    "\n",
    "#Action value keeper for maximizing action in policy improvement\n",
    "avk= np.zeros((25,4))\n",
    "\n",
    "\n",
    "# what_to_do= np.random.choice(actions, p=[0.25,0.25,0.25,0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1229f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(v_pi,states,actions, pi_s, gamma):\n",
    "    \n",
    "    #For comparing, with each iteration... we make a copy to hold next iterations state values\n",
    "    v= v_pi.copy()\n",
    "    \n",
    "    while True:\n",
    "        for s in states:\n",
    "            temp=0\n",
    "\n",
    "            #Pick an action in current states\n",
    "            for a in range(4):\n",
    "\n",
    "                act= actions[a]\n",
    "                R=-2 #Initializing reward to some value\n",
    "\n",
    "                #Figure out the next state s' and reward R, when you take action 'a' in state 's'\n",
    "\n",
    "\n",
    "                #Special cases\n",
    "                if(s==1):\n",
    "                    R=10\n",
    "                    s_dash=21\n",
    "\n",
    "                elif(s==3):\n",
    "                    R=5\n",
    "                    s_dash= 13\n",
    "\n",
    "\n",
    "                #If special cases not taken then look for Border cases. If we hit the boundary, we get -1 reward, but we state in present state.\n",
    "                if(R==-2):\n",
    "                    if(act=='left'):\n",
    "                        if(s%5==0):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='up'):\n",
    "                        if(s<5):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='right'):\n",
    "                        if(s%5==4):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='down'):\n",
    "                        if(s>19):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "\n",
    "                #If we haven't entered any special or border bouncing case then General Transitioning, with reward 0.\n",
    "                if(R==-2):\n",
    "\n",
    "                    R=0\n",
    "\n",
    "                    if(act=='up'):\n",
    "                        s_dash=s-5\n",
    "\n",
    "                    elif(act=='right'):\n",
    "                        s_dash= s+1\n",
    "\n",
    "                    elif(act=='down'):\n",
    "                        s_dash=s+5\n",
    "\n",
    "                    elif(act=='left'):\n",
    "                        s_dash=s-1\n",
    "\n",
    "                #Calculate v_pi(s) for each action 'a'\n",
    "                temp+= pi_s[s][a]*(1)*(R+ (gamma*v_pi[s_dash]))\n",
    "\n",
    "\n",
    "            v_pi[s]= temp\n",
    "\n",
    "\n",
    "        #Compare v_pi with v, after each iteration for all states 's'. \n",
    "        #We don't need v_pi-v for all 's'. This is a small set, we can look for equality and total convergence.\n",
    "        count=0\n",
    "        for x in range(25):\n",
    "            if(abs(v_pi[x]- v[x])<0.0001):\n",
    "                count+=1\n",
    "        if(count==25):\n",
    "            return v_pi\n",
    "\n",
    "        else:\n",
    "            v= v_pi.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9825757a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#For the initial policy [0,25,0.25,0.25,0.25]\n",
    "state_values= policy_evaluation(v_pi, states, actions, pi_s, gamma)\n",
    "\n",
    "\n",
    "#Policy iteration. \n",
    "while True:\n",
    "\n",
    "    v_pi= state_values.copy()\n",
    "    avk= np.zeros((25,4))\n",
    "    \n",
    "    #Policy Improvement\n",
    "    for s in range(25):\n",
    "        \n",
    "        for a in range(4):\n",
    "            act= actions[a]\n",
    "            R=-2 #Initializing reward to some value\n",
    "\n",
    "            #Figure out the next state s' and reward R, when you take action 'a' in state 's'\n",
    "\n",
    "\n",
    "            #Special cases\n",
    "            if(s==1):\n",
    "                R=10\n",
    "                s_dash=21\n",
    "\n",
    "            elif(s==3):\n",
    "                R=5\n",
    "                s_dash= 13\n",
    "\n",
    "\n",
    "            #If special cases not taken then look for Border cases. If we hit the boundary, we get -1 reward, but we state in present state.\n",
    "            if(R==-2):\n",
    "                if(act=='left'):\n",
    "                    if(s%5==0):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='up'):\n",
    "                    if(s<5):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='right'):\n",
    "                    if(s%5==4):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='down'):\n",
    "                    if(s>19):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "\n",
    "            #If we haven't entered any special or border bouncing case then General Transitioning, with reward 0.\n",
    "            if(R==-2):\n",
    "\n",
    "                R=0\n",
    "\n",
    "                if(act=='up'):\n",
    "                    s_dash=s-5\n",
    "\n",
    "                elif(act=='right'):\n",
    "                    s_dash= s+1\n",
    "\n",
    "                elif(act=='down'):\n",
    "                    s_dash=s+5\n",
    "\n",
    "                elif(act=='left'):\n",
    "                    s_dash=s-1\n",
    "  \n",
    "            \n",
    "    #When the policy has been evaluated, we now improve it.\n",
    "    #In each state we choose the state value maximizing action.\n",
    "    \n",
    "                  \n",
    "            \n",
    "            avk[s][a]=R+ (gamma*v_pi[s_dash])\n",
    "            \n",
    "    \n",
    "    #Now we edit pi_s as per greedy action in each state.\n",
    "    pi_s= np.zeros((25,4))\n",
    "    for i in states:\n",
    "        n=0\n",
    "    #If you find the greedy action, replace the value with a highly negative number and check of the next argmax gives the same max value as the previous one. So take that too and do for other two actions as well.\n",
    "    \n",
    "        temp=avk[i]\n",
    "        \n",
    "        max_ind=np.argmax(temp)\n",
    "        a0=temp[max_ind]\n",
    "        temp[max_ind]=-10000000\n",
    "        pi_s[i][max_ind]=1\n",
    "        n+=1\n",
    "        \n",
    "        max_ind= np.argmax(temp)\n",
    "        a1=temp[max_ind]\n",
    "        if(a0==a1):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "            \n",
    "        max_ind= np.argmax(temp)\n",
    "        a2=temp[max_ind]\n",
    "        if(a0==a2):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "\n",
    "            \n",
    "        max_ind= np.argmax(temp)\n",
    "        a3=temp[max_ind]\n",
    "        if(a0==a3):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "        \n",
    "     #Update the policy as per greedy selection.   \n",
    "        for j in range(4):\n",
    "            pi_s[i][j]= pi_s[i][j]/n\n",
    "            \n",
    "    \n",
    "    #Policy evaluation \n",
    "    #Run for the new policy, to find the state values\n",
    "    state_values= policy_evaluation(np.zeros(25), states, actions, pi_s, gamma)\n",
    "\n",
    "    \n",
    "    #If the state value from the new policy and old policy are close to each other for all states, then we found optimum policy.\n",
    "    counter=0\n",
    "    for i in states:\n",
    "        if(abs(state_values[i]-v_pi[i])==0):\n",
    "            counter+=1\n",
    "    if(counter==25):\n",
    "        break\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e81275f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.97741432, 24.41938153, 21.97744338, 19.41938153, 17.47744338,\n",
       "       19.77967288, 21.97744338, 19.77969904, 17.80172914, 16.02155622,\n",
       "       17.8017056 , 19.77969904, 17.80172914, 16.02155622, 14.4194006 ,\n",
       "       16.02153504, 17.80172914, 16.02155622, 14.4194006 , 12.97746054,\n",
       "       14.41938153, 16.02155622, 14.4194006 , 12.97746054, 11.67971449])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now pi_s is the optimum policy. So state_vals is v*(s)\n",
    "state_vals=policy_evaluation(v_pi, states, actions, pi_s, gamma)\n",
    "state_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9928e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ],\n",
       "       [0.5 , 0.5 , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[left, up, right ,down]...25 times for each state.\n",
    "#This is the optimum policy.\n",
    "pi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If instead of looking for convergence using state value, I run the pollicy iteration loop for 200 times, then I get the policy same as that of the book.\n",
    "# Here I got a different policy, but the state values are optimum as they are same as that of book.\n",
    "# We can have multiple optimum policies, but the optimum state values are fixed and same as that of book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
