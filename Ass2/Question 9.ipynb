{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80b182f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95ab0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "states= np.array(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "162dc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions= [ 'left' , 'up' , 'right' , 'down' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12354c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_s=np.ones((16,4))*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c62c376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avk= np.zeros((16,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d645d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(v_pi,states,actions, pi_s):\n",
    "    \n",
    "    #For comparing, with each iteration... we make a copy to hold next iterations state values\n",
    "    v= v_pi.copy()\n",
    "    \n",
    "    while True:\n",
    "        for s in range(1,15):\n",
    "            temp=0\n",
    "\n",
    "            #Pick an action in current states\n",
    "            for a in range(4):\n",
    "\n",
    "                act= actions[a]\n",
    "                R=-2 #Initializing reward to some value\n",
    "\n",
    "                #Figure out the next state s' and reward R, when you take action 'a' in state 's'\n",
    "\n",
    "                r=-1\n",
    "                #Terminal cases\n",
    "                if(s==1 and act=='left'):\n",
    "                    \n",
    "                    R=r\n",
    "                    s_dash=0\n",
    "\n",
    "                elif(s==4 and act=='up'):\n",
    "                    R=r\n",
    "                    s_dash= 0\n",
    "                    \n",
    "                elif(s==11 and act=='down'):\n",
    "                    R=r\n",
    "                    s_dash=15\n",
    "                \n",
    "                elif(s==14 and act=='right'):\n",
    "                    R=r\n",
    "                    s_dash=15\n",
    "\n",
    "\n",
    "                #If Terminal cases not taken then look for Border cases. If we hit the boundary, we get -1 reward, but we state in present state.\n",
    "                if(R==-2):\n",
    "                    if(act=='left'):\n",
    "                        if(s%4==0):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='up'):\n",
    "                        if(s<4):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='right'):\n",
    "                        if(s%4==3):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "                    elif(act=='down'):\n",
    "                        if(s>11):\n",
    "                            s_dash=s\n",
    "                            R=-1\n",
    "\n",
    "\n",
    "                #If we haven't entered any special or border bouncing case then General Transitioning, with reward 0.\n",
    "                if(R==-2):\n",
    "\n",
    "                    R=-1\n",
    "\n",
    "                    if(act=='up'):\n",
    "                        s_dash=s-4\n",
    "\n",
    "                    elif(act=='right'):\n",
    "                        s_dash= s+1\n",
    "\n",
    "                    elif(act=='down'):\n",
    "                        s_dash=s+4\n",
    "\n",
    "                    elif(act=='left'):\n",
    "                        s_dash=s-1\n",
    "\n",
    "                #Calculate v_pi(s) for each action 'a'\n",
    "                temp+= pi_s[s][a]*(R+ v_pi[s_dash])\n",
    "\n",
    "\n",
    "            v_pi[s]= temp\n",
    "\n",
    "\n",
    "        #Compare v_pi with v, after each iteration for all states 's'. \n",
    "        #We don't need v_pi-v for all 's'. This is a small set, we can look for equality and total convergence.\n",
    "        count=0\n",
    "        for x in range(16):\n",
    "            if(abs(v_pi[x]- v[x])<0.0001):\n",
    "                count+=1\n",
    "        if(count==16):\n",
    "            return v_pi\n",
    "\n",
    "        else:\n",
    "            v= v_pi.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf321d",
   "metadata": {},
   "source": [
    "# State Values for Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a172fb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99931242, -19.99901152, -21.99891199,\n",
       "       -13.99931242, -17.99915625, -19.99908389, -19.99909436,\n",
       "       -19.99901152, -19.99908389, -17.99922697, -13.99942284,\n",
       "       -21.99891199, -19.99909436, -13.99942284,   0.        ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pi= np.zeros((16))\n",
    "state_values= policy_evaluation(v_pi, states, actions, pi_s)\n",
    "state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daf80c",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11c49002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For the initial policy [0,25,0.25,0.25,0.25]\n",
    "state_values= policy_evaluation(v_pi, states, actions, pi_s)\n",
    "\n",
    "\n",
    "#Policy iteration. \n",
    "while True:\n",
    "    v_pi= state_values.copy()\n",
    "    avk= np.zeros((16,4))\n",
    "    \n",
    "    #Policy Improvement\n",
    "    for s in range(1,15):\n",
    "        \n",
    "        for a in range(4):\n",
    "            act= actions[a]\n",
    "            R=-2 #Initializing reward to some value\n",
    "\n",
    "\n",
    "            r=-1\n",
    "            #Terminal cases\n",
    "            if(s==1 and act=='left'):\n",
    "\n",
    "                R=r\n",
    "                s_dash=0\n",
    "\n",
    "            elif(s==4 and act=='up'):\n",
    "                R=r\n",
    "                s_dash= 0\n",
    "\n",
    "            elif(s==11 and act=='down'):\n",
    "                R=r\n",
    "                s_dash=15\n",
    "\n",
    "            elif(s==14 and act=='right'):\n",
    "                R=r\n",
    "                s_dash=15\n",
    "\n",
    "\n",
    "            #If special cases not taken then look for Border cases. If we hit the boundary, we get -1 reward, but we state in present state.\n",
    "            if(R==-2):\n",
    "                if(act=='left'):\n",
    "                    if(s%4==0):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='up'):\n",
    "                    if(s<4):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='right'):\n",
    "                    if(s%4==3):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "                elif(act=='down'):\n",
    "                    if(s>11):\n",
    "                        s_dash=s\n",
    "                        R=-1\n",
    "\n",
    "\n",
    "            #If we haven't entered any special or border bouncing case then General Transitioning, with reward 0.\n",
    "            if(R==-2):\n",
    "\n",
    "                R=-1\n",
    "\n",
    "                if(act=='up'):\n",
    "                    s_dash=s-4\n",
    "\n",
    "                elif(act=='right'):\n",
    "                    s_dash= s+1\n",
    "\n",
    "                elif(act=='down'):\n",
    "                    s_dash=s+4\n",
    "\n",
    "                elif(act=='left'):\n",
    "                    s_dash=s-1\n",
    "  \n",
    "            \n",
    "    #When the policy has been evaluated, we now improve it.\n",
    "    #In each state we choose the state value maximizing action.\n",
    "    \n",
    "                  \n",
    "            \n",
    "            avk[s][a]=R+ (v_pi[s_dash])\n",
    "            \n",
    "    \n",
    "    #Now we edit pi_s as per greedy action in each state.\n",
    "    pi_s= np.zeros((16,4))\n",
    "    for i in states:\n",
    "        n=0\n",
    "    #If you find the greedy action, replace the value with a highly negative number and check of the next argmax gives the same max value as the previous one. So take that too and do for other two actions as well.\n",
    "    \n",
    "        temp=avk[i]\n",
    "        \n",
    "        max_ind=np.argmax(temp)\n",
    "        a0=temp[max_ind]\n",
    "        temp[max_ind]=-10000000\n",
    "        pi_s[i][max_ind]=1\n",
    "        n+=1\n",
    "        \n",
    "        max_ind= np.argmax(temp)\n",
    "        a1=temp[max_ind]\n",
    "        if(a0==a1):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "            \n",
    "        max_ind= np.argmax(temp)\n",
    "        a2=temp[max_ind]\n",
    "        if(a0==a2):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "\n",
    "            \n",
    "        max_ind= np.argmax(temp)\n",
    "        a3=temp[max_ind]\n",
    "        if(a0==a3):\n",
    "            pi_s[i][max_ind]=1\n",
    "            n+=1\n",
    "            temp[max_ind]=-10000000\n",
    "        \n",
    "     #Update the policy as per greedy selection.   \n",
    "        for j in range(4):\n",
    "            pi_s[i][j]= pi_s[i][j]/n\n",
    "            \n",
    "    \n",
    "    #Policy evaluation \n",
    "    #Run for the new policy, to find the state values\n",
    "    state_values= policy_evaluation(np.zeros(16), states, actions, pi_s)\n",
    "\n",
    "    \n",
    "    #If the state value from the new policy and old policy are close to each other for all states, then we found optimum policy.\n",
    "    counter=0\n",
    "    for i in states:\n",
    "        if(abs(state_values[i]-v_pi[i])==0):\n",
    "            counter+=1\n",
    "    if(counter==16):\n",
    "        break\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "839008e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1., -2., -3., -1., -2., -3., -2., -2., -3., -2., -1., -3.,\n",
       "       -2., -1.,  0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now pi_s is the optimum policy. So state_vals is v*(s)\n",
    "state_vals=policy_evaluation(v_pi, states, actions, pi_s)\n",
    "state_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96d380bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.  , -1.  , -1.  , -1.  ],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.5 ,  0.  ,  0.  ,  0.5 ],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ],\n",
       "       [ 0.5 ,  0.5 ,  0.  ,  0.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.  ,  0.  ,  0.5 ,  0.5 ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.5 ,  0.5 ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [-1.  , -1.  , -1.  , -1.  ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Terminal states not needed\n",
    "pi_s[0]=[-1]*4\n",
    "pi_s[15]=[-1]*4\n",
    "pi_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
