{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91365d",
   "metadata": {},
   "source": [
    "# Create initial Rewards And Update it For other Time Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the initial true mean for all our arms.\n",
    "true_mean=np.random.randn(1)\n",
    "\n",
    "true_mean_for_ten_arms= [true_mean]*10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db55c3",
   "metadata": {},
   "source": [
    "# For Non constant Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4727a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#if two arms have the same Q value, then we will prefer the arm at the lower index.\n",
    "#players are independent of each other, hence we reset all values for every player.\n",
    "#Assuming 'e' will only have 2 decimal points. ==> 0.xy or 0.x form only\n",
    "e= 0.1\n",
    "\n",
    "trials=5000\n",
    "steps=1000\n",
    "\n",
    "R_time=np.zeros((10,steps)) #Sum of reward for every arm over 2000 trials at every time step t.\n",
    "N_time=np.zeros((10,steps)) #Number of times, an arm is picked at time 't' over all 2000 trials.\n",
    "\n",
    "\n",
    "\n",
    "#Keep record of Rt for every player, so you can calculate average Rt.\n",
    "Rt_for_every_player= np.zeros((trials,steps))\n",
    "\n",
    "#Generate a list, to get explore ith probabilty 'e' and 'exploit' with probability '1-e'.\n",
    "\n",
    "L=  ['exploit']*(int(100*(1-e))) + (['explore']*(int(100*e)))\n",
    "random.shuffle(L)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for player in range(trials):\n",
    "    \n",
    "    N=[0]*10\n",
    "    Q=[0]*10\n",
    "    true_mean_for_ten_arms_update= true_mean_for_ten_arms.copy()\n",
    "    \n",
    "    for t in range(steps):\n",
    "        \n",
    "        what_to_do_pos= np.random.randint(100)\n",
    "        what_to_do= L[what_to_do_pos]\n",
    "\n",
    "        if(what_to_do=='exploit'):\n",
    "            arm_to_pick= np.argmax(Q)\n",
    "\n",
    "        else:\n",
    "            arm_to_pick= np.random.randint(10)\n",
    "\n",
    "\n",
    "        #Now we got our reward at time step t.\n",
    "        Rt= np.random.normal(true_mean_for_ten_arms_update[arm_to_pick],1)\n",
    "\n",
    "        #Now we update the Q value for this arm which we picked\n",
    "        temp= Q[arm_to_pick]*N[arm_to_pick]\n",
    "        temp+=Rt\n",
    "        N[arm_to_pick]+=1\n",
    "        Q[arm_to_pick]= temp/N[arm_to_pick]\n",
    "\n",
    "        #Note the reward received at time 't' for every player.\n",
    "        Rt_for_every_player[player][t]=Rt\n",
    "        \n",
    "        #For plotting expected value for every arm as a function of time steps.\n",
    "        R_time[arm_to_pick][t]+=Rt\n",
    "        N_time[arm_to_pick][t]+=1\n",
    "        \n",
    "       \n",
    "\n",
    "        for k in range(10):\n",
    "            true_mean_for_ten_arms_update[k]+= np.random.normal(0, 0.01)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "    print(player, \"is Done\")\n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246b91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_reward_list_0=[]\n",
    "for t in range(steps):\n",
    "    total_reward_at_t=0\n",
    "    for player in range(trials):\n",
    "\n",
    "        total_reward_at_t+=Rt_for_every_player[player][t]\n",
    "\n",
    "\n",
    "    average_reward_at_t= total_reward_at_t/trials\n",
    "\n",
    "    average_reward_list_0.append(average_reward_at_t)\n",
    "plt.figure(0)\n",
    "plt.style.use('dark_background')\n",
    "plt.title('Average_rweard for time step with non stationary reward distributions', color='orange')\n",
    "plt.yticks(color='orange')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.xticks(color='orange')\n",
    "plt.plot(range(steps), average_reward_list_0)\n",
    "plt.show()\n",
    "\n",
    "print('The method is performing soo poorly over a non stationary reward distribution. We never sort of saturate to higher average rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e26db",
   "metadata": {},
   "source": [
    "# For Constant alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2f308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#if two arms have the same Q value, then we will prefer the arm at the lower index.\n",
    "#players are independent of each other, hence we reset all values for every player.\n",
    "#Assuming 'e' will only have 2 decimal points. ==> 0.xy or 0.x form only\n",
    "e= 0.1\n",
    "a=0.1\n",
    "trials=5000\n",
    "steps=1000\n",
    "\n",
    "R_time=np.zeros((10,steps)) #Sum of reward for every arm over 2000 trials at every time step t.\n",
    "N_time=np.zeros((10,steps)) #Number of times, an arm is picked at time 't' over all 2000 trials.\n",
    "\n",
    "\n",
    "\n",
    "#Keep record of Rt for every player, so you can calculate average Rt.\n",
    "Rt_for_every_player= np.zeros((trials,steps))\n",
    "\n",
    "#Generate a list, to get explore ith probabilty 'e' and 'exploit' with probability '1-e'.\n",
    "L=  ['exploit']*(int(100*(1-e))) + (['explore']*(int(100*e)))\n",
    "random.shuffle(L)\n",
    "\n",
    "\n",
    "for players in range(trials):\n",
    "    \n",
    "    N=[0]*10\n",
    "    \n",
    "    Q=[1]*10\n",
    "    \n",
    "#     reward_history= [R]*10\n",
    "    \n",
    "    true_mean_for_ten_arms_update= true_mean_for_ten_arms.copy()\n",
    "    \n",
    "    for t in range(steps):\n",
    "        \n",
    "        index_for_L= np.random.randint(len(L))\n",
    "        what_to_do= L[index_for_L]\n",
    "        \n",
    "        if(what_to_do== 'explore'):\n",
    "            At= np.random.randint(10)\n",
    "            \n",
    "        else:\n",
    "            At= np.argmax(Q)\n",
    "            \n",
    "        \n",
    "        Rt= np.random.normal(true_mean_for_ten_arms_update[At],1) #Get the reward Rt\n",
    "        N[At]+=1                                                  #Increment the number of times, the arm has been picked.\n",
    "        \n",
    "#         reward_history[At][N[At]]= Rt #What reward did we get from At, when we picked it for n times in this trial.\n",
    "        \n",
    "        \n",
    "        Rt_for_every_player[players][t]= Rt #Reward received by players at time step 't'\n",
    "        \n",
    "        R_time[At][t]+=Rt  #Total reward receievd by an arm at some time 't' over all trials\n",
    "        N_time[At][t]+=1   #Total times an arm was picked over all trials at some time 't'.\n",
    "        \n",
    "        \n",
    "       #Update Q value\n",
    "    \n",
    "        Q[At]= Q[At]+ (a*(Rt-Q[At])) \n",
    "        \n",
    "    \n",
    "    print('player', players, 'done')\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859013c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward_list_1=[]\n",
    "for t in range(steps):\n",
    "    total_reward_at_t=0\n",
    "    for player in range(trials):\n",
    "\n",
    "        total_reward_at_t+=Rt_for_every_player[player][t]\n",
    "\n",
    "\n",
    "    average_reward_at_t= total_reward_at_t/trials\n",
    "\n",
    "    average_reward_list_1.append(average_reward_at_t)\n",
    "plt.figure(0)\n",
    "plt.style.use('dark_background')\n",
    "plt.title('Average_rweard for time step with non stationary reward distributions and constant a', color='orange')\n",
    "plt.yticks(color='orange')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.xticks(color='orange')\n",
    "plt.plot(range(steps), average_reward_list_1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
